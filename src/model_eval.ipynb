{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize information about trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Does dropping correlated features lead to a better model? (no)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                best_cv_acc  test_acc  test_precision  test_recall   test_f1  \\\n",
      "model                                                                          \n",
      "knn                0.003044  0.009091        0.036782     0.018182  0.021261   \n",
      "logreg_elastic    -0.001081  0.000000        0.000000     0.000000  0.000000   \n",
      "logreg_l2          0.003215 -0.018182       -0.035714    -0.054545 -0.056982   \n",
      "rfc               -0.009502  0.009091        0.006667     0.018182  0.017647   \n",
      "svc                0.002162 -0.009091       -0.003333     0.036364  0.003890   \n",
      "xgb                0.001991  0.009091        0.029212    -0.018182 -0.003896   \n",
      "\n",
      "                test_roc_auc  \n",
      "model                         \n",
      "knn             4.462810e-02  \n",
      "logreg_elastic  2.220446e-16  \n",
      "logreg_l2      -1.322314e-02  \n",
      "rfc            -6.611570e-03  \n",
      "svc            -4.958678e-03  \n",
      "xgb             5.785124e-02  \n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "def load_agg(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    df[\"metric\"] = df[\"metric\"].astype(str)\n",
    "\n",
    "    return df.set_index(\"metric\")\n",
    "\n",
    "# compare deltas of model accuracy across dfs with correlated cols deleted vs not deleted\n",
    "def compare_agg_folders(no_corr_dir: str, corr_dir: str, out_p: str = \"../results/deltas.csv\"):\n",
    "    no_files = {p.name: p for p in Path(no_corr_dir).glob(\"*_agg.csv\")}\n",
    "    co_files = {p.name: p for p in Path(corr_dir).glob(\"*_agg.csv\")}\n",
    "    \n",
    "    common = sorted(set(no_files) & set(co_files))\n",
    "    metric_cols = [\"best_cv_acc\", \"test_acc\", \"test_precision\", \"test_recall\", \"test_f1\", \"test_roc_auc\"]\n",
    "\n",
    "    wide_rows = {}\n",
    "    for fname in common:\n",
    "        df_no = load_agg(no_files[fname])\n",
    "        df_co = load_agg(co_files[fname])\n",
    "\n",
    "        # delta_mean = no_corr - corr\n",
    "        delta_mean = df_no[\"mean\"] - df_co[\"mean\"]\n",
    "\n",
    "        model = fname.replace(\"_agg.csv\", \"\")\n",
    "        wide_rows[model] = delta_mean.reindex(metric_cols)\n",
    "\n",
    "    deltas = pd.DataFrame.from_dict(wide_rows, orient=\"index\")\n",
    "    deltas.index.name = \"model\"\n",
    "    deltas.to_csv(out_p)\n",
    "\n",
    "    return deltas\n",
    "\n",
    "deltas = compare_agg_folders(no_corr_dir=\"../results/no_ftrs_dropped\", corr_dir=\"../results/ftrs_dropped\",)\n",
    "\n",
    "print(deltas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find top two models for primary and secondary evaluation metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model          metric      mean       std\n",
      "0              rfc     best_cv_acc  0.579603  0.010483\n",
      "1              xgb     best_cv_acc  0.576204  0.015965\n",
      "2              svc     best_cv_acc  0.565788  0.007257\n",
      "3        logreg_l2     best_cv_acc  0.562295  0.015133\n",
      "4   logreg_elastic     best_cv_acc  0.560556  0.019175\n",
      "5              knn     best_cv_acc  0.553193  0.012936\n",
      "6              svc        test_acc  0.569231  0.055403\n",
      "7              rfc        test_acc  0.557692  0.080448\n",
      "8        logreg_l2        test_acc  0.550000  0.075222\n",
      "9   logreg_elastic        test_acc  0.546154  0.061718\n",
      "10             knn        test_acc  0.507692  0.064645\n",
      "11             xgb        test_acc  0.484615  0.053363\n",
      "12             svc         test_f1  0.524075  0.123695\n",
      "13             rfc         test_f1  0.501671  0.143305\n",
      "14       logreg_l2         test_f1  0.485011  0.145865\n",
      "15  logreg_elastic         test_f1  0.479988  0.131420\n",
      "16             xgb         test_f1  0.437024  0.164438\n",
      "17             knn         test_f1  0.368538  0.140527\n",
      "18             svc  test_precision  0.550607  0.055465\n",
      "19             rfc  test_precision  0.533929  0.101063\n",
      "20       logreg_l2  test_precision  0.520486  0.096538\n",
      "21  logreg_elastic  test_precision  0.520204  0.087197\n",
      "22             knn  test_precision  0.476424  0.089026\n",
      "23             xgb  test_precision  0.446048  0.082151\n",
      "24             svc     test_recall  0.520000  0.176635\n",
      "25             rfc     test_recall  0.488000  0.190578\n",
      "26       logreg_l2     test_recall  0.464000  0.177989\n",
      "27             xgb     test_recall  0.464000  0.239332\n",
      "28  logreg_elastic     test_recall  0.456000  0.161493\n",
      "29             knn     test_recall  0.320000  0.154919\n",
      "30             rfc    test_roc_auc  0.581926  0.065485\n",
      "31             svc    test_roc_auc  0.577185  0.075899\n",
      "32  logreg_elastic    test_roc_auc  0.566222  0.072461\n",
      "33       logreg_l2    test_roc_auc  0.561778  0.085222\n",
      "34             knn    test_roc_auc  0.484741  0.080709\n",
      "35             xgb    test_roc_auc  0.469778  0.066696\n"
     ]
    }
   ],
   "source": [
    "# find best 2 models for each eval metric\n",
    "\n",
    "root = Path(\"../results/final\") \n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in root.rglob(\"*_agg.csv\"):\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    model = p.stem.replace(\"_agg\", \"\")\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        rows.append({\"model\": model, \"metric\": r[\"metric\"], \"mean\": r[\"mean\"], \"std\": r[\"std\"]})\n",
    "\n",
    "all_metrics = pd.DataFrame(rows)\n",
    "\n",
    "per_metric = (all_metrics\n",
    "    .sort_values([\"metric\", \"mean\", \"std\"], ascending=[True, False, True])\n",
    "    .groupby(\"metric\", as_index=False).head(10).reset_index(drop=True))\n",
    "\n",
    "per_metric.to_csv('../results/final/best_per_metric.csv', index=False)\n",
    "print(per_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
